{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d821e8e4",
   "metadata": {},
   "source": [
    "# Client\n",
    "\n",
    "在 AutoGen 中，我们需要将使用的 LLM 配置到 Client 中，然后才能调用 LLM 构建智能体，所以如何编写和使用 Client 是我们的第一步。\n",
    "\n",
    "本节主要介绍如何通过编写及使用 `Client`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae12492",
   "metadata": {},
   "source": [
    "## 如何保存你的 API key\n",
    "\n",
    "在调用模型 API 时最重要的是安全的保存我们的 API key，所以首先介绍两种常用的存储 API key 的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cde845",
   "metadata": {},
   "source": [
    "法一：在代码中直接设置（建议测试用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"skxxxxxxxxxxxx\" # 你的 api key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bf612",
   "metadata": {},
   "source": [
    "法二：写入 `.env` 文件（推荐）\n",
    "\n",
    "该方法将 API key 存储到运行程序外，较为安全，适合我们进行测试开发，**后面的演示我们将都使用该方法**。\n",
    "\n",
    "当有有多个 key 时，取不同的名字做区分即可。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc2efc47",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# .env 文件示例\n",
    "OPENAI_API_KEY=sk-openai-key-123\n",
    "SILICONFLOW_API_KEY=sk-siliconflow-key-456\n",
    "OTHER_API_KEY=sk-other-key-789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # 读取你的 OPENAI API key\n",
    "\n",
    "print(\"读取到的 API Key 是：\", api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb3c44",
   "metadata": {},
   "source": [
    "> 当然以上两种都是方便我们测试使用的方法，在实际的生产环境中，更多的是将 API key 存储到环境变量中，这里不做赘述。\n",
    "> 想继续了解的可以参考这篇文章进行学习：https://zhuanlan.zhihu.com/p/627665725"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619796d9",
   "metadata": {},
   "source": [
    "## 如何编写 Client\n",
    "\n",
    "`ChatCompletionClient` 是 Autogen 框架中用于封装与大模型交互的客户端类，专门用于 Chat Completion API 的请求，支持 OpenAI API 兼容的模型服务（如 OpenAI、Azure OpenAI、Anthropic、Gemini、硅基流动等）。\n",
    "\n",
    "`autogen_core.models.ChatCompletionClient` 是**抽象基类，不能直接实例化**。所以我们必须选择其子类进行实例化，这里我们选择使用 `OpenAIChatCompletionClient` 这一最常用的子类。\n",
    "\n",
    "> *亲测，`autogen=0.9.0` 是不能调用 `autogen.ChatCompletionClient` 的可以不必尝试了。*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e6803",
   "metadata": {},
   "source": [
    "### OpenAIChatCompletionClient\n",
    "\n",
    "`OpenAIChatCompletionClient` 是基于 `ChatCompletionClient` 编写的专门对接 OpenAI 这类 API 的子类实现。在接入第三方平台的 API 时，除了更换对应的 url 外还需要添加使用的模型的 `modelinfo`。\n",
    "\n",
    "`modelinfo` 的作用是说明你调用的模型的基本属性，需要时去对应平台的官方文档或 API文档查看模型名称列表，照着填就可以。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003090ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接调用 OPENAI 的 API \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"openai_API_KEY\") # 读取你的 API key\n",
    "\n",
    "openai_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",                          # 模型名称\n",
    "    base_url=\"https://api.openai.com/v1\",    # API 基础地址\n",
    "    api_key=\"你的API密钥\"                     # API 密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688a0bb",
   "metadata": {},
   "source": [
    "在调用非 OpenAI 的 API 时需要补充模型对应的 `modelinfo`，表格中的属性是必填项。\n",
    "\n",
    "| 字段名                   | 类型   |含义                                                      | 示例或推荐值            |\n",
    "|--------------------------|--------|---------------------------------------------------------|------------------------|\n",
    "| `family`                 | `str`  |  模型族，方便内部归类                                      | `\"qwen\"` 或 `\"openai\"` |\n",
    "| `context_length`         | `int`  |  上下文最大 token 长度                                     | `8192`（看模型规格）   |\n",
    "| `max_output_tokens`      | `int`  |  单次生成最大 tokens 数                                    | `2048`                 |\n",
    "| `tool_choice_supported`  | `bool` |  是否支持 OpenAI 的 `tool_choice` 功能（函数调用选择）       | `False`                |\n",
    "| `tool_choice_required`   | `bool` |  是否强制要求 `tool_choice`                                 | `False`                |\n",
    "| `structured_output`      | `bool` |  是否支持结构化输出（如 JSON 输出能力）                    | `False`                |\n",
    "| `vision`                 | `bool` |  是否支持图片输入/多模态能力                                | `False`                |\n",
    "| `function_calling`       | `bool` |  是否支持 `function_call`（函数调用能力）                   | `False`                |\n",
    "| `json_output`            | `bool` |  是否原生支持 JSON 格式输出                                 | `False`                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38aa5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用非 OPENAI 的 API \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "load_dotenv()\n",
    "other_api_key = os.getenv(\"OTHER_API_KEY\") # 读取你的 API key\n",
    "\n",
    "other_client = OpenAIChatCompletionClient(\n",
    "    model=\"\",                           # 模型名称\n",
    "    base_url=\"\",                        # API 基础地址\n",
    "    api_key=\"你的API密钥\",               # API 密钥\n",
    "    model_info={\n",
    "        \"family\": \"qwen\",              \n",
    "        \"context_length\": 8192,        \n",
    "        \"max_output_tokens\": 2048,     \n",
    "        \"tool_choice_supported\": False, \n",
    "        \"tool_choice_required\": False,  \n",
    "        \"structured_output\": False,     \n",
    "        \"vision\": False,                \n",
    "        \"function_calling\": False,      \n",
    "        \"json_output\": True           \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b2ff9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 硅基流动\n",
    "\n",
    "下面演示如何使用 `OpenAIChatCompletionClient` 调用硅基流动的 API。\n",
    "\n",
    "硅基流动平台的 API 获取可参考该链接：https://zhuanlan.zhihu.com/p/21156769766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77375977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "load_dotenv()\n",
    "siliconflow_api_key = os.getenv(\"SILICONFLOW_API_KEY\") # 读取你的 OPENAI API key\n",
    "\n",
    "# 初始化 OpenAIChatCompletionClient 客户端，连接到硅基流动平台的 Qwen3-8B 模型\n",
    "siliconflow_client = OpenAIChatCompletionClient(\n",
    "    model=\"Qwen/Qwen3-8B\",                # 指定要调用的模型名称，硅基流动平台上 Qwen 3-8B 模型\n",
    "    base_url=\"https://api.siliconflow.cn/v1\",  # 硅基流动平台的 API 访问地址\n",
    "    api_key=siliconflow_api_key,  # 你的 API 密钥\n",
    "    model_info={                        \n",
    "        \"family\": \"qwen\",              \n",
    "        \"context_length\": 8192,        \n",
    "        \"max_output_tokens\": 2048,     \n",
    "        \"tool_choice_supported\": True, \n",
    "        \"tool_choice_required\": False,  \n",
    "        \"structured_output\": True,     \n",
    "        \"vision\": False,                \n",
    "        \"function_calling\": False,      \n",
    "        \"json_output\": True           \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02fbf3",
   "metadata": {},
   "source": [
    "### Openrouter\n",
    "\n",
    "下面演示如何简单使用`OpenAIChatCompletionClient`调用OpenRouter的API。\n",
    "\n",
    "OpenRouter平台的API获取可参考链接：https://zhuanlan.zhihu.com/p/28203837581\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321bdd0f-dca1-4004-a813-d60513b7d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "load_dotenv()\n",
    "siliconflow_api_key = os.getenv(\"OPENROUTER_API_KEY\") # 读取你的 OPENAI API key\n",
    "\n",
    "# 初始化 OpenAIChatCompletionClient 客户端，连接到硅基流动平台的 Qwen3-8B 模型\n",
    "siliconflow_client = OpenAIChatCompletionClient(\n",
    "    model=\"qwen/qwen3-14b:free\",               \n",
    "    base_url=\"https://openrouter.ai/api/v1\",       # OpenRouter的 API 访问地址\n",
    "    api_key=siliconflow_api_key,  # 你的 API 密钥\n",
    "    model_info={                        \n",
    "        \"family\": \"qwen\",              \n",
    "        \"context_length\": 8192,        \n",
    "        \"max_output_tokens\": 2048,     \n",
    "        \"tool_choice_supported\": True, \n",
    "        \"tool_choice_required\": False,  \n",
    "        \"structured_output\": True,     \n",
    "        \"vision\": False,                \n",
    "        \"function_calling\": False,      \n",
    "        \"json_output\": True           \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297f261",
   "metadata": {},
   "source": [
    "## 如何使用 Client\n",
    "\n",
    "在使用 `OpenAIChatCompletionClient` 创建 Client 后，下面将讲解其配套的方法。\n",
    "\n",
    "| 方法                | 功能说明                                                                                           |\n",
    "| ------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| `create()`          | 向模型发送一组消息，获取一次性完整回复。参数通常为消息对象列表（如 UserMessage），返回模型生成的结果。适用于标准问答、对话等场景。<br>**示例：**<br>`result = await client.create([UserMessage(content=\"你好\", source=\"user\")])` |\n",
    "| `create_stream()`   | 向模型发送消息，获取流式（分段）回复。适合长文本或需要实时展示生成内容的场景。通常以异步生成器方式返回内容片段。<br>**示例：**<br>`async for chunk in client.create_stream([...]): print(chunk)` |\n",
    "| `count_tokens()`    | 统计一段文本或一组消息的 token 数量。用于控制输入长度、避免超出模型最大上下文限制。<br>**示例：**<br>`num = client.count_tokens(\"你好，世界\")` 或 `num = client.count_tokens([UserMessage(...)])` |\n",
    "| `close()`           | 关闭客户端，释放网络连接等资源。建议在所有请求结束后调用，避免资源泄漏。<br>**示例：**<br>`await client.close()` |\n",
    "| `model_info`        | 属性，返回当前模型的配置信息（如模型名称、上下文长度、支持的功能等），通常为字典格式。<br>**示例：**<br>`print(client.model_info)` |\n",
    "| `capabilities`      | 属性，返回模型的能力描述（如是否支持函数调用、流式输出、图片输入等），用于判断模型支持的高级特性。<br>**示例：**<br>`print(client.capabilities)` |\n",
    "\n",
    "---\n",
    "\n",
    "**说明：**  \n",
    "- `create()` 和 `create_stream()` 是最常用的生成方法，分别对应一次性输出和流式输出。\n",
    "- `count_tokens()` 有助于输入长度管理。\n",
    "- `model_info` 和 `capabilities` 便于了解和判断模型的详细能力。\n",
    "- `close()` 用于资源管理，尤其在异步环境下非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40478e29",
   "metadata": {},
   "source": [
    "### create()\n",
    "\n",
    "`OpenAIChatCompletionClient.create()` 用于发起一次完整的对话请求，异步获取生成的完整响应。\n",
    "\n",
    "> 使用场景：普通对话、文本生成、总结、问答\n",
    "\n",
    "- 用途：向大模型发起一次完整的对话请求，获取模型的最终生成结果。\n",
    "\n",
    "- 异步：该方法是一个异步协程，需要在异步函数中使用 await。\n",
    "\n",
    "- 兼容：与 OpenAI 原生 chat/completions API 保持一致，但加入了 autogen 层的统一参数管理。\n",
    "\n",
    "| 参数名             | 类型         | 说明                                                         |\n",
    "|--------------------|--------------|--------------------------------------------------------------|\n",
    "| messages           | 列表         | 对话消息对象列表（必须为 `SystemMessage`、`UserMessage`、`AssistantMessage` 或 `FunctionExecutionResultMessage`）         |\n",
    "| tools              | 列表，可选   | 可用工具或工具描述，支持函数调用等高级功能                   |\n",
    "| json_output        | bool/类型，可选 | 是否要求模型输出 JSON，或指定输出的 Pydantic 模型类型        |\n",
    "| extra_create_args  | 字典，可选   | 额外参数（如 temperature、max_tokens 等）                    |\n",
    "| cancellation_token | 对象，可选   | 用于取消请求的令牌     |\n",
    "\n",
    "\n",
    "\n",
    "**返回值**：返回一个 `CreateResult` 对象，包含模型生成的回复内容及相关信息。\n",
    "\n",
    "\n",
    "参考：https://microsoft.github.io/autogen/dev/reference/python/autogen_core.models.html#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2340a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='AutoGen框架是由微软开发的一种**多智能体协作的AI框架**，旨在通过模拟多个AI智能体之间的协作实现更复杂、更高效的任务处理和决策过程。它的核心理念是让AI角色（如助手、角色扮演者、用户代理等）在对话中根据任务需求动态分配角色和职责，从而完成人机协同工作的目标。\\n\\n---\\n\\n### **1. 架构特点**\\nAutoGen的架构基于**多智能体系统（Multi-Agent Systems）**，允许不同AI智能体（Agent）之间通过对话进行交互。主要特点包括：\\n- **角色多样性**：每个智能体可以被赋予特定角色（如研究者、表演者、口语者、检验者等），并根据角色设定不同的行为模式和能力。\\n- **分布式协作**：智能体可以并行运行，通过消息传递机制进行任务分工和结果汇总。\\n- **动态任务分配**：根据任务需求，框架能自动决定哪些角色参与、如何角色切换。\\n- **支持多种模型**：兼容大语言模型（LLMs）如GPT、LLaMA、Baichuan等，并可以结合现有工具（如API、数据库）使用。\\n\\n---\\n\\n### **2. 核心组件**\\n#### **(1) 智能体（Agent）**\\n- **用户代理（UserProxyAgent）**：用于引导或干预智能体之间的对话。\\n- **助手代理（AssistantAgent）**：执行任务的核心代理，基于LLM生成响应。\\n- **角色代理（ConversableAgent）**：可自定义角色的代理，实现多角色交互（如研究员、审稿人等）。\\n- **群组代理（GroupChat）**：管理多个智能体之间的协作流程，实现**多智能体会话**（Group Chat）。\\n\\n#### **(2) 工作流程**\\n- **初始化**：定义智能体及其角色、职责和目标。\\n- **任务分派**：用户代理或群组代理分配任务。\\n- **智能体协作**：各代理根据角色生成回复，可能进行多轮对话或分工。\\n- **结果输出**：最终结果由用户代理或指定代理汇总并返回。\\n\\n---\\n\\n### **3. 主要功能**\\n1. **会话管理（Conversation Management）**  \\n   支持复杂对话流程，如多跳对话（multiple hops）、角色切换和引导用户输入。\\n   \\n2. **任务分派与规划（Task Delegation & Planning）**  \\n   智能体可以动态规划任务，将复杂任务拆分为子任务并分配给其他代理。\\n\\n3. **手持代理（User Proxy）**  \\n   通过UserProxyAgent，用户可以在对话中实时干预，提供反馈或修改任务方向。\\n\\n4. **提示优化（Prompt Tuning）**  \\n   提供工具对提示词进行优化，帮助智能体生成更准确的响应。\\n\\n5. **自定义角色与行为**  \\n   可通过脚本定义新的角色（如客服、分析师）并分配其动态行为。\\n\\n---\\n\\n### **4. 应用场景**\\nAutoGen适用于需要**多角色协作**的复杂任务，例如：\\n- **教育**：模拟老师与学生的互动（如生成教学内容、答疑、作文批改）。\\n- **企业自动化**：AI客服与数据分析师协作处理客户查询。\\n- **科研**：多个智能体分工完成文献调研、实验设计与论文写作。\\n- **数据分析**：将数据处理、模型训练和结果解释分给不同代理。\\n- **内容创作**：编剧、演员、导演角色共同完成剧本或创意项目。\\n\\n---\\n\\n### **5. 实现案例**\\n- **AI导师与学生**：用户与AI导师进行对话，导师调用其他智能体（如数学、计算机科学家）解答问题。\\n- **论文写作**：研究员主角生成大纲，审稿人代理提出修改建议，最终通过多轮协作完成论文。\\n- **客服系统**：AI客服代理处理查询，涉及多个子代理（如账单查询、技术支持）协作解决问题。\\n\\n---\\n\\n### **6. 优势**\\n- **复用性强**：支持模块化设计，可灵活组合不同功能的代理。\\n- **可扩展性**：可加入外部工具、API或数据库查询，实现现实世界任务。\\n- **人机交互自然**：通过对话接口，用户可直接参与流程，减少技术门槛。\\n- **开源社区支持**：框架完全开源（GitHub），已有大量社区贡献和模块（如代码解释器角色），方便二次开发。\\n\\n---\\n\\n### **7. 与传统AI关键区别**\\n| 特性            | AutoGen               | 传统AI应用          |\\n|-----------------|------------------------|----------------------|\\n| **协作模式**     | 多智能体动态协作       | 单智能体处理任务     |\\n| **任务分解**     | 自动拆分任务并分派     | 任务需人工预定义     |\\n| **交互方式**     | 通过对话自然交互       | 需通过代码或接口调用 |\\n| **灵活性**       | 支持多角色动态切换     | 通常固定功能         |\\n| **应用场景**     | 复杂多步任务（如科研） | 简单任务（如文本生成）|\\n\\n---\\n\\n### **8. 主要挑战**\\n- **调试复杂**：多智能体间的对话流程可能难以预测。\\n- **依赖LLM性能**：效果高度依赖基础模型（如GPT）的能力。\\n- **伦理问题**：需注意角色设定的合理性，避免误导性内容。\\n\\n---\\n\\n### **9. 开发示例（伪代码）**\\n```python\\nfrom autogen import UserProxyAgent, AssistantAgent, GroupChat\\n\\n# 定义角色\\nresearcher = AssistantAgent(name=\"Researcher\", llm_config={\"model\": \"gpt-4\"})\\nchecker = AssistantAgent(name=\"Checker\", llm_config={\"model\": \"gpt-3.5-turbo\"})\\nuser_proxy = UserProxyAgent(name=\"User\", human_input_mode=\"ALWAYS\")\\n\\n# 创建群组并分配规则\\ngroup_chat = GroupChat(agents=[researcher, checker], messages=[], max_turns=5)\\nmanager = GroupChatManager(group_chat=group_chat)\\n\\n# 开始任务\\nuser_proxy.initiate_chat(manager, message=\"如何解决气候变化问题？\")\\n```\\n\\n---\\n\\n### **10. 动态角色切换机制**\\nAutoGen允许智能体在对话中主动请求角色切换，例如：\\n- **研究员**请求助手分析数据。\\n- **助手**调用**代码解释器**生成结果。\\n- **审查者**检查结果并要求修改。\\n\\n这种机制通过定制化工作流规则（如`max_turns`或`allowed_speaker_transitions`）实现。\\n\\n---\\n\\n### **11. 社区与生态**\\n- **开源项目**：AutoGen在GitHub上托管，支持快速迭代。\\n- **模块化扩展**：社区贡献的角色（如`code_interpreter`）可直接用于实际任务。\\n- **兼容性**：与LangChain、LlamaIndex等主流框架无缝衔接。\\n\\n---\\n\\n### **12. 适用人群**\\n- **开发者**：希望构建多角色协作系统的AI工程师。\\n- **研究人员**：需要实验不同AI角色行为的研究人员。\\n- **企业用户**：希望自动化复杂业务流程（如文档生成、客服）的企业。\\n\\n---\\n\\n### **总结**\\nAutoGen通过**模拟多智能体对话**，显著提升了AI在复杂任务中的表现。其核心价值在于：\\n- 解决单模型难以处理的多步骤/多角色问题；\\n- 展示了AI协作的可能性和效率，强调了**流程设计**与**角色定义**对任务效果的关键作用。  \\n不过，使用时需注意LCM机制的设计和LLM的调优，以确保对话的连贯性和针对性。' usage=RequestUsage(prompt_tokens=15, completion_tokens=2157) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "\n",
    "result = await siliconflow_client.create([\n",
    "    UserMessage(content=\"请介绍一下AutoGen框架。\", source=\"user\")\n",
    "])\n",
    "print(result)\n",
    "await siliconflow_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5cf4f",
   "metadata": {},
   "source": [
    "### count_tokens（）\n",
    "\n",
    "`count_tokens()` 方法用于统计一段文本或一组消息在当前模型下会被分成多少个 token（分词单元）。  \n",
    "这对于控制输入长度、避免超过模型最大上下文限制非常重要。\n",
    "\n",
    "**常见应用场景：**\n",
    "\n",
    "- 输入长度控制：确保输入不会超过模型的最大 token 限制（如 8192）。\n",
    "- 动态截断：根据 token 数量自动截断过长的输入。\n",
    "- 计费估算：部分模型按 token 数量计费，可用此方法预估消耗。\n",
    "\n",
    "**注意事项：**\n",
    "\n",
    "- 不同模型的分词方式不同，同样的文本在不同模型下 token 数量可能不同。\n",
    "- 该方法通常为同步方法（即不用 await），但具体实现请参考实际 Client 文档。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6374671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model qwen/qwen3-14b:free not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "\n",
    "# 用 UserMessage 包装你的文本，放到列表里\n",
    "num = siliconflow_client.count_tokens([UserMessage(content=\"你好，AutoGen！\", source=\"user\")])\n",
    "print(num)  # 输出 token 数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "843a5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model qwen/qwen3-14b:free not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# 统计多条消息的 token 数量\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "messages = [\n",
    "    UserMessage(content=\"你好\", source=\"user\"),\n",
    "    UserMessage(content=\"请介绍一下AutoGen。\", source=\"user\")\n",
    "]\n",
    "\n",
    "num = siliconflow_client.count_tokens(messages)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92d097",
   "metadata": {},
   "source": [
    "### create_stream()\n",
    "\n",
    "`create_stream()` 是 LLM Client 的流式生成方法，用于将消息发送给大语言模型，并**实时分段返回**生成内容。  \n",
    "适合长文本生成、实时展示、边生成边处理等场景。\n",
    "\n",
    "**应用场景：**\n",
    "\n",
    "- **长文本生成**：如代码生成、文档写作、小说创作等。\n",
    "- **实时对话**：聊天机器人、智能客服等需要即时响应的场景。\n",
    "- **前端流式展示**：网页、终端等界面实时显示生成内容。\n",
    "\n",
    "\n",
    "**主要特点**\n",
    "\n",
    "- **流式输出**：模型生成内容时，客户端可以一边接收一边处理，无需等待全部生成完毕。\n",
    "- **异步生成器**：通常以 `async for` 的方式遍历每个内容片段。\n",
    "- **节省等待时间**：用户体验更好，适合对响应速度要求高的应用。\n",
    "\n",
    "**注意事项**\n",
    "\n",
    "- 需要在支持异步的环境下使用（如 Jupyter Notebook、异步脚本等）。\n",
    "- 消息参数格式与 `create()` 方法一致，需传入消息对象列表。\n",
    "- 不同模型和 Client 对流式支持程度不同，具体请查阅对应文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f7a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGen 是由微软亚洲研究院（MSRA）开源的一套 **多智能体协作框架**，旨在通过自动化以及智能体之间的协作来简化大规模 AI 应用开发，同时提升 AI 剧本编写和执行的效率。其核心目标是让用户能够更专注于业务逻辑，而无需手动编码处理复杂的 AI 协作流程。\n",
      "\n",
      "---\n",
      "\n",
      "## 一、核心理念\n",
      "\n",
      " **AI 剧本**（AI Playbooks）概念，即通过预定义角色、行为规则、任务目标和协作流程，使 **多个 AI 智能体（Agent）** 能够自动化地完成复杂的任务。它强调以下几点：\n",
      "\n",
      "1. **模块化和可扩展性**  \n",
      "   每个智能体（Agent）可以独立开发、测试和部署，同时通过框架提供接口进行协作。\n",
      "\n",
      "2. **角色驱动的行为设计**  \n",
      "，通过角色间的交互实现动态流程控制。\n",
      "\n",
      "自动化任务执行**  \n",
      "   智能体自动完成任务，例如数据收集、问题解答、代码生成等。\n",
      "\n",
      "4. **基于 LLM 的动态决策**  \n",
      "智能体的行为可以通过大语言模型（LLM）的输出动态调整，从而实现灵活的协作。\n",
      "\n",
      "---\n",
      "\n",
      "## 二、主要组件\n",
      "\n",
      " 框架由以下核心组件构成：\n",
      "\n",
      "### 1. **Role（角色）**\n",
      "每个智能体都有一个角色，角色包含以下属性：\n",
      "：表示该智能体在协作中要完成的任务。\n",
      "外部环境或其它角色互动。n）**：角色如何与\n",
      "生成行为决策。制（Reasoning）**：角色如何利用 LLM \n",
      "\n",
      "#### 常见角色类型：\n",
      "：负责与用户交互，收集需求，并在无需 AI 参与时完成操作（如发送完成信号）。\n",
      "文本、执行推理等。tantAgent（助理代理）**：负责处理一般任务，如回答问题、生成\n",
      "话的启动点，负责初始化角色并控制流程的开始。gent（会话起点代理）**：作为协作会\n",
      "）**：用于管理多个代理人之间的动态对话，支持多角色协作。\n",
      "）。**LLM（大语言模型）**：用户自定义使用的语言模型（如 GPT、Qwen、RAG 等\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Flow Management（流程管理）**\n",
      "Termination Condition）** 和 **行为规划（Action Planning）** 的流程控制机制，用于协调多个 AI 智能体的协作顺序和终止条件。\n",
      "\n",
      "#### 关键概念：\n",
      "动态调整智能体之间的对话流程，例如强制指定某些智能体执行某些子任务。\n",
      "角色管理器（Conversation Role Manager）**：管理会话中角色的加入与退出，支持平行、交替、反应式等协作模式。\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Message Passing（消息传递）**\n",
      "体之间通过 **消息队列（Message Queue）** 进行通信，消息包含以下字段：\n",
      "- **内容（Content）**：消息的具体内容（如文本、代码、数据等）。\n",
      "发送者（Sender）**：消息的来源智能体。\n",
      "消息的接收者。（Receiver）**：\n",
      "PPT 等。类型（Message Type）**：如文本、代码、表格、\n",
      "（如调用外部数据库、调用 API 等）。或 API 接口\n",
      "\n",
      "，**AutoGen 支持基于 LLM 的消息生成策略**，例如：\n",
      "- 用户代理生成需求。\n",
      "- 助理代理生成响应。\n",
      "（如运行代码）。完成特定操作\n",
      "\n",
      "---\n",
      "\n",
      " 的规划）****LLM-Driven Planning（基于 LLM\n",
      "AutoGen 允许开发者编写 **提示词（Prompt）** 来指导 LLM 输出特定的行为决策，例如：\n",
      "- \"作为助理代理，帮我分析这份数据，并生成一个图表。\"\n",
      "代理，假如您有一个问题，请将该问题拆解给各个助理代理，并协调他们提供解决方案。\"\n",
      "\n",
      "这种规划机制让智能体能够独立完成多层次任务，并自动触发下一步。\n",
      "\n",
      "---\n",
      "\n",
      " **Distributed Execution（分布式执行）**\n",
      "计算节点/资源（如 REMOTE、LOCAL、Distributed）中执行智能体。例如：\n",
      "- 使用弹性计算 API 或调用本地脚本。\n",
      "- 允许每个智能体运行在独立的模块中，以避免信息泄露（如 Low-Level Data）和提高资源利用率。\n",
      "\n",
      "---\n",
      "\n",
      "## 三、使用场景\n",
      "\n",
      "以下场景：en 框架特别适用于\n",
      "\n",
      "### 1. **AI 工程落地**\n",
      "如从数据库中提取数据并由助理代理进行分析。\n",
      "协作生成用户文档、教程、甚至是摘要。\n",
      "将编程需求拆解，多个助理代理在同一数据集上生成代码。\n",
      "\n",
      "研发协作中的 Application Writing**\n",
      " assistants 一起编写一个复杂程序，并在实行结果后进一步优化。\n",
      "结构进行推理，并自动修正错误或生成 debug 日志。\n",
      "\n",
      "### 3. **BBQ（Business Brain and Knowledge）型 AI 应用**\n",
      "- **多角色协作**：如用户代理可以与政府发言人、律师、财务顾问等多个助理代理进行交互。\n",
      "给另一个助手智能体进行整合或执行。在一个智能体处理完一个问题后，自动转移\n",
      "\n",
      "### 4. **对话 AI 系统优化**\n",
      "- **地点级对话管理**：基于 LLM 的 “**termed-presence-based**”عقب四模型relationships，实现复杂的对话结构。\n",
      "代理回答用户后，自动将记录保存，并根据预先设定的智能体进行下一步处理（如保荐前或加入新的代理）。\n",
      "\n",
      "---\n",
      "\n",
      "## 四、集成与扩展\n",
      "\n",
      "支持灵活的集成与扩展，以下是一些关键特点：\n",
      "\n",
      "Chain 的优化接口**ang\n",
      "API 调用、代码生成器等）构建更复杂的 AI 系统。PI，这允许开发者通过模块化的组件（如数据库、\n",
      "\n",
      "（OpenAI API, LocalLLM Architectures, Model ZOO）**\n",
      "开发者可以：\n",
      "等）。用 OpenAI 的 GPT 模型（如 GPT-4、Azure GPT \n",
      "- 使用本地部署（如 Mixtral 模型）。\n",
      "OO，引入不同架构的 AI 模型（如 DAMO Academy、SignLab、LLaMA 等）。\n",
      "\n",
      ". **自动化与 man-in-the-loop（人机协作）的支持**\n",
      "流程控制器中的规则，实现 AI 的自主行为。\n",
      "行为决策过程中植入人机协作节点，例如在某一步提醒用户做出决策。\n",
      "\n",
      "---\n",
      "\n",
      "AutoGen 的优势\n",
      "\n",
      "1. **减少开发复杂度**  \n",
      " AI 系统的编写和集成，使开发者可以专注于业务需求。\n",
      "\n",
      "   **提高资源利用率**\n",
      "。  每个智能体可以采用不同的资源，如计算节点、模型架构，最大化效率\n",
      "\n",
      "3. **灵活的协作机制**  \n",
      "模型的分布式处理，充分利用 LLM 的潜力。 \n",
      "\n",
      "4. **适应多种应用场景**  \n",
      "   可用于文字段落-level AI，也可用于代码-level AI，广泛适用于各类多角色 AI 系统。\n",
      "\n",
      "---\n",
      "\n",
      "六、挑战与未来\n",
      "\n",
      "：然 AutoGen 在 AI 分布式平台上取得重大进展，但仍存在一些挑战\n",
      "\n",
      "。 **稳定性与安全保障**：多方交互模块化可能带来数据泄露或模型反应不可预测的问题\n",
      "主行为中，是未来研究的重点。何更好地将人类决策融入 AI 自\n",
      "- **实时性要求**：在某些业务流程中，AI 的响应速度需要进一步优化。\n",
      "支持 Model Zoo 的优劣**：目前 AutoGen 对 Model Zoo 的支持虽然已具备，但进一步优化插件系统可提升适用范围。\n",
      "\n",
      "---\n",
      "\n",
      "## 七、对比其他框架\n",
      "\n",
      "| Framework | Focus | Component Design | LLM Integration | Use Cases |\n",
      "---|---||---|\n",
      " AI Product Dev, Automation |Agent Collaboration | Role-Driven, Flow-Controllable | LLMs + Code, API |\n",
      " Modular | Riders, Info Apps |ystem Building | Component-Based, Extractive |\n",
      " AI, VR || Dialogue AI | Text-based, open world | General | Dialog\n",
      "| Simple Agent | Lightweight use | Minimal roles | Adapted | Small AI systems |\n",
      "\n",
      "Gen 的独特之处在于其 **深度整合 Multiple AI models（如 GPT、MiniCPM、DeepSeek、Llama3 等）**，并引入复杂的剧本系统，使 AI Collaboration 在 NLP、Agent、Automation 等多个维度都获得提升。\n",
      "\n",
      "---\n",
      "\n",
      "（简化）、实际使用示例\n",
      "\n",
      "```python\n",
      "from autogen import UserProxyAgent, AssistantAgent, GroupChat\n",
      "\n",
      " 初始化两个助理代理\n",
      "pt-4\", \"temperature\": 0.4})(name=\"assistant1\", llm_config={\"model\": \"g\n",
      " llm_config={\"model\": \"gpt-4\", \"temperature\": 0.3})\n",
      "\n",
      "用户代理\n",
      "end\" in x[\"content\"])yAgent(name=\"user_proxy\", is_termination_msg=lambda x: \"\n",
      "\n",
      "# 构建协作组\n",
      "1, assistant2], messages=[], max_turns=3)\n",
      "\n",
      " 启动会话\n",
      "user_proxy.initiate_chat(group_chat, message=\"请帮我生成一首中文诗\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 九、总结\n",
      "\n",
      "AutoGen 是微软亚洲研究院推出的 **AI 多智能体协作系统框架**，通过角色的定义、流程控制和 DDL（Data-Driven Logic）设计，使不同 AI models 能够跨角色、跨系统、跨数据执行复杂任务。它的灵活性和多场景适配性使其成为 AI 工程和自动化应用的有力工具。\n",
      "\n",
      "型的系统，或者需要启动一个自动化 AI 流程，那么 AutoGen 是一个非常值得考虑的框架。finish_reason='stop' content='AutoGen 是由微软亚洲研究院（MSRA）开源的一套 **多智能体协作框架**，旨在通过自动化以及智能体之间的协作来简化大规模 AI 应用开发，同时提升 AI 剧本编写和执行的效率。其核心目标是让用户能够更专注于业务逻辑，而无需手动编码处理复杂的 AI 协作流程。\\n\\n---\\n\\n## 一、核心理念\\n\\nAutoGen 的设计灵感来源于 **AI 剧本**（AI Playbooks）概念，即通过预定义角色、行为规则、任务目标和协作流程，使 **多个 AI 智能体（Agent）** 能够自动化地完成复杂的任务。它强调以下几点：\\n\\n1. **模块化和可扩展性**  \\n   每个智能体（Agent）可以独立开发、测试和部署，同时通过框架提供接口进行协作。\\n\\n2. **角色驱动的行为设计**  \\n   为每个智能体分配特定角色和目标，通过角色间的交互实现动态流程控制。\\n\\n3. **自动化任务执行**  \\n   智能体自动完成任务，例如数据收集、问题解答、代码生成等。\\n\\n4. **基于 LLM 的动态决策**  \\n   智能体的行为可以通过大语言模型（LLM）的输出动态调整，从而实现灵活的协作。\\n\\n---\\n\\n## 二、主要组件\\n\\nAutoGen 框架由以下核心组件构成：\\n\\n### 1. **Role（角色）**\\n每个智能体都有一个角色，角色包含以下属性：\\n- **目标（Goal）**：表示该智能体在协作中要完成的任务。\\n- **行为（Action）**：角色如何与外部环境或其它角色互动。\\n- **决策机制（Reasoning）**：角色如何利用 LLM 生成行为决策。\\n\\n#### 常见角色类型：\\n- **UserProxyAgent（用户代理）**：负责与用户交互，收集需求，并在无需 AI 参与时完成操作（如发送完成信号）。\\n- **AssistantAgent（助理代理）**：负责处理一般任务，如回答问题、生成文本、执行推理等。\\n- **ConversationStartAgent（会话起点代理）**：作为协作会话的启动点，负责初始化角色并控制流程的开始。\\n- **GroupChat（会话组）**：用于管理多个代理人之间的动态对话，支持多角色协作。\\n- **LLM（大语言模型）**：用户自定义使用的语言模型（如 GPT、Qwen、RAG 等）。\\n\\n---\\n\\n### 2. **Flow Management（流程管理）**\\nAutoGen 提供了基于 **收尾程序（Termination Condition）** 和 **行为规划（Action Planning）** 的流程控制机制，用于协调多个 AI 智能体的协作顺序和终止条件。\\n\\n#### 关键概念：\\n- **流程控制器（Flow Controller）**：用于动态调整智能体之间的对话流程，例如强制指定某些智能体执行某些子任务。\\n- **会话角色管理器（Conversation Role Manager）**：管理会话中角色的加入与退出，支持平行、交替、反应式等协作模式。\\n\\n---\\n\\n### 3. **Message Passing（消息传递）**\\n智能体之间通过 **消息队列（Message Queue）** 进行通信，消息包含以下字段：\\n- **内容（Content）**：消息的具体内容（如文本、代码、数据等）。\\n- **发送者（Sender）**：消息的来源智能体。\\n- **接收者（Receiver）**：消息的接收者。\\n- **消息类型（Message Type）**：如文本、代码、表格、PPT 等。\\n- **工具（Tool）**：附加的工具或 API 接口（如调用外部数据库、调用 API 等）。\\n\\n此外，**AutoGen 支持基于 LLM 的消息生成策略**，例如：\\n- 用户代理生成需求。\\n- 助理代理生成响应。\\n- 通过工具调用完成特定操作（如运行代码）。\\n\\n---\\n\\n### 4. **LLM-Driven Planning（基于 LLM 的规划）**\\nAutoGen 允许开发者编写 **提示词（Prompt）** 来指导 LLM 输出特定的行为决策，例如：\\n- \"作为助理代理，帮我分析这份数据，并生成一个图表。\"\\n- \"用户代理，假如您有一个问题，请将该问题拆解给各个助理代理，并协调他们提供解决方案。\"\\n\\n这种规划机制让智能体能够独立完成多层次任务，并自动触发下一步。\\n\\n---\\n\\n### 5. **Distributed Execution（分布式执行）**\\nAutoGen 支持在不同计算节点/资源（如 REMOTE、LOCAL、Distributed）中执行智能体。例如：\\n- 使用弹性计算 API 或调用本地脚本。\\n- 允许每个智能体运行在独立的模块中，以避免信息泄露（如 Low-Level Data）和提高资源利用率。\\n\\n---\\n\\n## 三、使用场景\\n\\nAutoGen 框架特别适用于以下场景：\\n\\n### 1. **AI 工程落地**\\n- **自动化数据处理**：如从数据库中提取数据并由助理代理进行分析。\\n- **自动化内容创作**：智能体协作生成用户文档、教程、甚至是摘要。\\n- **自动化代码生成**：例如，用户代理将编程需求拆解，多个助理代理在同一数据集上生成代码。\\n\\n### 2. **研发协作中的 Application Writing**\\n- **联合代码优化**：例如，Multiple AI assistants 一起编写一个复杂程序，并在实行结果后进一步优化。\\n- **链式推理与调试**：多个助理智能体可以以链式结构进行推理，并自动修正错误或生成 debug 日志。\\n\\n### 3. **BBQ（Business Brain and Knowledge）型 AI 应用**\\n- **多角色协作**：如用户代理可以与政府发言人、律师、财务顾问等多个助理代理进行交互。\\n- **角色切换与整合**：例如，在一个智能体处理完一个问题后，自动转移给另一个助手智能体进行整合或执行。\\n\\n### 4. **对话 AI 系统优化**\\n- **地点级对话管理**：基于 LLM 的 “**termed-presence-based**”عقب四模型relationships，实现复杂的对话结构。\\n- **角色应对与反馈闭环**：例如，在一个客服代理回答用户后，自动将记录保存，并根据预先设定的智能体进行下一步处理（如保荐前或加入新的代理）。\\n\\n---\\n\\n## 四、集成与扩展\\n\\nAutoGen 框架支持灵活的集成与扩展，以下是一些关键特点：\\n\\n### 1. **基于 LangChain 的优化接口**\\nAutoGen 集成了 LangChain 提供的 API，这允许开发者通过模块化的组件（如数据库、API 调用、代码生成器等）构建更复杂的 AI 系统。\\n\\n### 2. **支持多种 LLM 接入（OpenAI API, LocalLLM Architectures, Model ZOO）**\\n开发者可以：\\n- 使用 OpenAI 的 GPT 模型（如 GPT-4、Azure GPT 等）。\\n- 使用本地部署（如 Mixtral 模型）。\\n- 接入 Model ZOO，引入不同架构的 AI 模型（如 DAMO Academy、SignLab、LLaMA 等）。\\n\\n### 3. **自动化与 man-in-the-loop（人机协作）的支持**\\n- **完全自动化**：通过设定流程控制器中的规则，实现 AI 的自主行为。\\n- **man-in-the-loop**：在 AI 的行为决策过程中植入人机协作节点，例如在某一步提醒用户做出决策。\\n\\n---\\n\\n## 五、AutoGen 的优势\\n\\n1. **减少开发复杂度**  \\n   AutoGen 简化了 AI 系统的编写和集成，使开发者可以专注于业务需求。\\n\\n2. **提高资源利用率**  \\n   每个智能体可以采用不同的资源，如计算节点、模型架构，最大化效率。\\n\\n3. **灵活的协作机制**  \\n   通过角色和流程控制机制，整合多个 AI 模型的分布式处理，充分利用 LLM 的潜力。\\n\\n4. **适应多种应用场景**  \\n   可用于文字段落-level AI，也可用于代码-level AI，广泛适用于各类多角色 AI 系统。\\n\\n---\\n\\n## 六、挑战与未来\\n\\n虽然 AutoGen 在 AI 分布式平台上取得重大进展，但仍存在一些挑战：\\n\\n- **稳定性与安全保障**：多方交互模块化可能带来数据泄露或模型反应不可预测的问题。\\n- **人机协作平衡**：如何更好地将人类决策融入 AI 自主行为中，是未来研究的重点。\\n- **实时性要求**：在某些业务流程中，AI 的响应速度需要进一步优化。\\n- **当前支持 Model Zoo 的优劣**：目前 AutoGen 对 Model Zoo 的支持虽然已具备，但进一步优化插件系统可提升适用范围。\\n\\n---\\n\\n## 七、对比其他框架\\n\\n| Framework | Focus | Component Design | LLM Integration | Use Cases |\\n|---|---|---|---|---|\\n| AutoGen | AI Power through Agent Collaboration | Role-Driven, Flow-Controllable | LLMs + Code, API | AI Product Dev, Automation |\\n| LangChain | Modularized AI System Building | Component-Based, Extractive | Modular | Riders, Info Apps |\\n| ParlAI | Dialogue AI | Text-based, open world | General | Dialog AI, VR |\\n| Simple Agent | Lightweight use | Minimal roles | Adapted | Small AI systems |\\n\\nAutoGen 的独特之处在于其 **深度整合 Multiple AI models（如 GPT、MiniCPM、DeepSeek、Llama3 等）**，并引入复杂的剧本系统，使 AI Collaboration 在 NLP、Agent、Automation 等多个维度都获得提升。\\n\\n---\\n\\n## 八、实际使用示例（简化）\\n\\n```python\\nfrom autogen import UserProxyAgent, AssistantAgent, GroupChat\\n\\n# 初始化两个助理代理\\nassistant1 = AssistantAgent(name=\"assistant1\", llm_config={\"model\": \"gpt-4\", \"temperature\": 0.4})\\nassistant2 = AssistantAgent(name=\"assistant2\", llm_config={\"model\": \"gpt-4\", \"temperature\": 0.3})\\n\\n# 创建用户代理\\nuser_proxy = UserProxyAgent(name=\"user_proxy\", is_termination_msg=lambda x: \"end\" in x[\"content\"])\\n\\n# 构建协作组\\ngroup_chat = GroupChat(agents=[assistant1, assistant2], messages=[], max_turns=3)\\n\\n# 启动会话\\nuser_proxy.initiate_chat(group_chat, message=\"请帮我生成一首中文诗\")\\n```\\n\\n---\\n\\n## 九、总结\\n\\nAutoGen 是微软亚洲研究院推出的 **AI 多智能体协作系统框架**，通过角色的定义、流程控制和 DDL（Data-Driven Logic）设计，使不同 AI models 能够跨角色、跨系统、跨数据执行复杂任务。它的灵活性和多场景适配性使其成为 AI 工程和自动化应用的有力工具。\\n\\n如果你正计划构建一个包含多个 AI 模型的系统，或者需要启动一个自动化 AI 流程，那么 AutoGen 是一个非常值得考虑的框架。' usage=RequestUsage(prompt_tokens=15, completion_tokens=2967) cached=False logprobs=None thought=None"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "\n",
    "messages = [UserMessage(content=\"请详细介绍AutoGen框架。\", source=\"user\")]\n",
    "\n",
    "# 流式获取模型输出\n",
    "async for chunk in siliconflow_client.create_stream(messages):\n",
    "    print(chunk, end=\"\")  # 每次输出一段内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d6937",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html#autogen_core.models.ChatCompletionClient\n",
    "2. https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/models.html\n",
    "3. https://microsoft.github.io/autogen/dev/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient\n",
    "4. https://docs.siliconflow.cn/cn/api-reference/chat-completions/chat-completions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
