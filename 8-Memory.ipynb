{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64576468",
   "metadata": {},
   "source": [
    "# Memory \n",
    "\n",
    "在许多应用场景中，我们希望维护一个“知识存储”，在特定步骤之前智能地将其中的事实加入智能体上下文中。\n",
    "一个典型的例子就是 RAG 模式（Retrieval-Augmented Generation）——通过查询检索数据库中的相关信息，然后将其动态注入到智能体的上下文中。\n",
    "\n",
    "AutoGen 提供了一个可扩展的 `Memory` 协议接口，来实现上述能力。它包括以下关键方法：\n",
    "\n",
    "| 方法名              | 功能说明                                                 |\n",
    "| ---------------- | ---------------------------------------------------------  |\n",
    "| `add`            | 向记忆库中添加新条目                                         |\n",
    "| `query`          | 从记忆库中检索与当前任务相关的信息                            |\n",
    "| `update_context` | 修改智能体的 `model_context`，将检索到的信息动态加入（由 `AssistantAgent` 使用）|\n",
    "| `clear`          | 清空记忆库中的所有内容                                     |\n",
    "| `close`          | 清理记忆库使用的资源（如文件句柄、数据库连接等）            |\n",
    "\n",
    "目前官方文档中已有的针对不同存储类型的Memory类：\n",
    "\n",
    "1. ListMemory  \n",
    "\n",
    "`ListMemory` 简单的基于列表的内存实现 Memory 功能，按时间顺序维护内存，并将最近的内存追加到模型的上下文中。该实现设计得简单明了、可预测，因此易于理解和调试。 \n",
    "\n",
    "2. ChromaDBVectorMemory\n",
    "\n",
    "`ChromaDBVectorMemory`使用由 ChromaDB 支持的矢量相似性搜索存储和检索内存。ChromaDBVectorMemory \"提供了一种基于向量的内存实现，它使用 ChromaDB存储和检索基于语义相似性的内容。 它增强了代理在对话过程中调用上下文相关信息的能力。   \n",
    "\n",
    "3. Mem0Memory\n",
    "\n",
    "`Mem0Memory` 提供与 Mem0.ai 内存系统的集成。 它支持云端和本地后端，为代理提供高级内存功能。 该实现可处理适当的检索和上下文更新，因此适用于生产环境。\n",
    "\n",
    "除此之外，我们还可以通过继承`Memory`基类以及重写其中的方法来实现我们想要存储记忆方式，例如，您可以实现一个使用向量数据库来存储和检索信息的自定义存储空间，或者一个使用机器学习模型来根据用户偏好等生成个性化响应的存储空间。我们将会以构建基于`Vector DBs`的存储方式的Memory系统为例对这一重要部分进行讲解。\n",
    "\n",
    "另外，我们可基于Memory的功能下\n",
    "✅ 应用场景示例：\n",
    "- 🌐 RAG 查询文档数据库，支持问答或摘要任务\n",
    "\n",
    "- 🤖 将过去对话片段或用户设定动态注入到当前轮上下文\n",
    "\n",
    "- 📁 将用户上传的知识库（如 PDF、网页）作为记忆源动态接入\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b683d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "load_dotenv()\n",
    "siliconflow_api_key = os.getenv(\"SILICONFLOW_API_KEY\") # 读取你的 OPENAI API key\n",
    "\n",
    "# 初始化 OpenAIChatCompletionClient 客户端，连接到硅基流动平台的 Qwen3-8B 模型\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"Qwen/Qwen3-14B\",                # 指定要调用的模型名称，硅基流动平台上 Qwen 3-8B 模型\n",
    "    base_url=\"https://api.siliconflow.cn/v1\",  # 硅基流动平台的 API 访问地址\n",
    "    api_key=siliconflow_api_key,  # 你的 API 密钥\n",
    "    model_info={                        \n",
    "        \"family\": \"qwen\",              \n",
    "        \"context_length\": 8192,        \n",
    "        \"max_output_tokens\": 2048,     \n",
    "        \"tool_choice_supported\": True, \n",
    "        \"tool_choice_required\": False,  \n",
    "        \"structured_output\": True,     \n",
    "        \"vision\": False,                \n",
    "        \"function_calling\": True,      \n",
    "        \"json_output\": True,\n",
    "        \"multiple_system_messages\":True\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f24e85-3646-4992-b69d-e825b38c7c1a",
   "metadata": {},
   "source": [
    "我们要介绍autogen_core.memory下的两个类`ListMemoryConfig`和 `ListMemory`，方便之后的其他类型的Memory讲解\n",
    "\n",
    "### ListMemoryConfig\n",
    "\n",
    "`ListMemoryConfig`是 Autogen 框架中ListMemory组件的配置类，基于 Pydantic 的`BaseModel`构建，用于验证和序列化内存组件的配置参数。其核心作用是：\n",
    "\n",
    "1. 定义ListMemory的初始化参数规范\n",
    "2. 支持配置的序列化与反序列化（如 JSON 转换）\n",
    "3. 提供类型提示和参数验证\n",
    "\n",
    "关键属性说明：\n",
    "\n",
    "`name`属性\n",
    "\n",
    "类型：str | None\n",
    "\n",
    "作用：为内存实例指定唯一名称，便于在多组件场景中识别\n",
    "\n",
    "默认值：None（自动生成默认名称\"default_list_memory\"）\n",
    "\n",
    "`memory_contents`属性\n",
    "\n",
    "类型：List[MemoryContent]\n",
    "\n",
    "作用：初始化时预加载的内存内容列表\n",
    "\n",
    "默认值：通过default_factory=list生成空列表\n",
    "\n",
    "注意：MemoryContent是 Autogen 的内存内容模型，包含content、metadata、mime_type等字段\n",
    "\n",
    "我们来看官方文档里面的源代码会对其定义有更好的了解：\n",
    "\n",
    "class ListMemoryConfig(BaseModel):\n",
    "    \"\"\"Configuration for ListMemory component.\"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    \"\"\"Optional identifier for this memory instance.\"\"\"\n",
    "    memory_contents: List[MemoryContent] = Field(default_factory=list)\n",
    "    \"\"\"List of memory contents stored in this memory instance.\"\"\"\n",
    "\n",
    "### ListMemory\n",
    "\n",
    "`ListMemory`是 Autogen 框架中基于列表的内存实现，用于存储和管理记忆内容，遵循Memory协议并集成Component配置系统。它以 chronological（时间顺序）方式存储内容，并支持将记忆内容整合到模型上下文中。\n",
    "\n",
    "源代码中对`ListMemory`类的构造函数如下：\n",
    "\n",
    "def __init__(self, name: str | None = None, memory_contents: List[MemoryContent] | None = None) -> None:\n",
    "        self._name = name or \"default_list_memory\"\n",
    "        self._contents: List[MemoryContent] = memory_contents if memory_contents is not None else []\n",
    "\n",
    "其中第二个参数memory_contents就可以传入上面我们所讲的针对ListMemory的统一配置`ListMemoryConfig`，使得初始化的ListMemory实例都能获得ListMemoryConfig已经配备好的信息。\n",
    "\n",
    "以下是一个带`ListMemory`记忆功能的智能体运用例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563cb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "# Initialize user memory\n",
    "user_memory = ListMemory()\n",
    "\n",
    "# Add user preferences to memory\n",
    "await user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "\n",
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 °F and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 °C and Sunny.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't know the weather in {city}.\"\n",
    "\n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    memory=[user_memory],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e35587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is the weather in New York?\n",
      "---------- MemoryQueryEvent (assistant_agent) ----------\n",
      "[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n",
      "---------- ThoughtEvent (assistant_agent) ----------\n",
      "\n",
      "\n",
      "\n",
      "---------- ToolCallRequestEvent (assistant_agent) ----------\n",
      "[FunctionCall(id='0197c0adc898da07ad2e5e7026bad2dc', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')]\n",
      "---------- ToolCallExecutionEvent (assistant_agent) ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='0197c0adc898da07ad2e5e7026bad2dc', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (assistant_agent) ----------\n",
      "The weather in New York is 23 °C and Sunny.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='What is the weather in New York?', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, metadata={}, content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)], type='MemoryQueryEvent'), ThoughtEvent(source='assistant_agent', models_usage=None, metadata={}, content='\\n\\n', type='ThoughtEvent'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=733, completion_tokens=118), metadata={}, content=[FunctionCall(id='0197c0adc898da07ad2e5e7026bad2dc', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='0197c0adc898da07ad2e5e7026bad2dc', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, metadata={}, content='The weather in New York is 23 °C and Sunny.', type='ToolCallSummaryMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent with a task.\n",
    "stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c158eae1-fa6b-4202-9381-ae5acd33756e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n",
       " SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n",
       " AssistantMessage(content=[FunctionCall(id='0197c09c432fe83c0acceea5353fc584', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')], thought='\\n\\n', source='assistant_agent', type='AssistantMessage'),\n",
       " FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='0197c09c432fe83c0acceea5353fc584', is_error=False)], type='FunctionExecutionResultMessage'),\n",
       " UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n",
       " SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n",
       " AssistantMessage(content=[FunctionCall(id='0197c09f2125fe930799ce7cf4943125', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')], thought='\\n\\n', source='assistant_agent', type='AssistantMessage'),\n",
       " FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='0197c09f2125fe930799ce7cf4943125', is_error=False)], type='FunctionExecutionResultMessage')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant_agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa508df2",
   "metadata": {},
   "source": [
    "我们可以观察到，`assistant_agent` 的 `model_context` 实际上已经被检索到的记忆条目更新了。`transform` 方法用于将这些记忆条目格式化为一个代理可以使用的字符串。在这种情况下，我们只是将每个记忆条目的内容简单地连接成一个字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e32485",
   "metadata": {},
   "source": [
    "我们可以看到，上述返回的天气信息是以摄氏度为单位的，正如用户偏好中所设定的那样。\n",
    "\n",
    "同样地，假设我们提出一个关于生成饮食计划的问题，代理也能够从记忆存储中检索到相关信息，并提供一个个性化的（素食）回应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd8d78-084b-4b7d-a17a-386789dc3a58",
   "metadata": {},
   "source": [
    "讲完了ListMemory的基本用法，我们简单举一下 `ChromaDBVectorMemory` 和 `Mem0Memory` 的例子，他们的用法大同小异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862cee7-247d-4bb5-bc82-319e7b303a33",
   "metadata": {},
   "source": [
    "### ChromaDBVectorMemory\n",
    "\n",
    "`ChromaDBVectorMemory`将我们传入的知识库进行向量化后存储在ChromaDB中，所以要传入你想要的Embedding方法，目前官方源代码中定义了以下几个EmbeddingFunctionConfig\n",
    "\n",
    "1. SentenceTransformerEmbeddingFunctionConfig\n",
    "\n",
    "2. OpenAIEmbeddingFunctionConfig\n",
    "\n",
    "3. DefaultEmbeddingFunctionConfig\n",
    "\n",
    "4. CustomEmbeddingFunctionConfig\n",
    "\n",
    "他们的区别是所使用的Embedding模型不一样。\n",
    "\n",
    "建议用户直接前往autogen_ext/memory/chromadb/_chroma_configs.py(V0.6.2)中查看源代码中的定义，这样你会有更清晰的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661f41e-0dc8-4694-8dc7-6bc88c8590e5",
   "metadata": {},
   "source": [
    "### ChromaDBVectorMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3fe02-2a82-4237-9caf-cabf01af56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import MemoryContent, MemoryMimeType\n",
    "from autogen_ext.memory import chromadb\n",
    "# (\n",
    "#     ChromaDBVectorMemory,\n",
    "#     PersistentChromaDBVectorMemoryConfig,\n",
    "#     SentenceTransformerEmbeddingFunctionConfig,\n",
    "# )\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Use a temporary directory for ChromaDB persistence\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    chroma_user_memory = ChromaDBVectorMemory(\n",
    "        config=PersistentChromaDBVectorMemoryConfig(\n",
    "            collection_name=\"preferences\",\n",
    "            persistence_path=tmpdir,  # Use the temp directory here\n",
    "            k=2,  # Return top k results\n",
    "            score_threshold=0.4,  # Minimum similarity score\n",
    "            embedding_function_config=SentenceTransformerEmbeddingFunctionConfig(\n",
    "                model_name=\"all-MiniLM-L6-v2\"  # Use default model for testing\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # Add user preferences to memory\n",
    "    await chroma_user_memory.add(\n",
    "        MemoryContent(\n",
    "            content=\"The weather should be in metric units\",\n",
    "            mime_type=MemoryMimeType.TEXT,\n",
    "            metadata={\"category\": \"preferences\", \"type\": \"units\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    await chroma_user_memory.add(\n",
    "        MemoryContent(\n",
    "            content=\"Meal recipe must be vegan\",\n",
    "            mime_type=MemoryMimeType.TEXT,\n",
    "            metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    # Create assistant agent with ChromaDB memory\n",
    "    assistant_agent = AssistantAgent(\n",
    "        name=\"assistant_agent\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_weather],\n",
    "        memory=[chroma_user_memory],\n",
    "    )\n",
    "\n",
    "    stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n",
    "    await Console(stream)\n",
    "\n",
    "    # await model_client.close()\n",
    "    # await chroma_user_memory.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c22a9a-81c7-4dcb-9968-f0879e374b4b",
   "metadata": {},
   "source": [
    "### Mem0Memory\n",
    "\n",
    "注意：Mem0Memory在0.6.2及之后的版本才有的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90fa9e-4b1f-4461-ae2c-6afb4ffb8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import uuid\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, TypedDict, cast\n",
    "\n",
    "from autogen_core import CancellationToken, Component, ComponentBase\n",
    "from autogen_core.memory import Memory, MemoryContent, MemoryQueryResult, UpdateContextResult\n",
    "from autogen_core.model_context import ChatCompletionContext\n",
    "from autogen_core.models import SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Self\n",
    "\n",
    "try:\n",
    "    from mem0 import Memory as Memory0\n",
    "    from mem0 import MemoryClient\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"`mem0ai` not installed. Please install it with `pip install mem0ai`\") from e\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
    " # Create a cloud Mem0Memory\n",
    "memory = Mem0Memory(is_cloud=True)\n",
    "# Add something to memory\n",
    "await memory.add(MemoryContent(content=\"Important information to remember\"))\n",
    "# Retrieve memories with a search query\n",
    "results = await memory.query(\"relevant information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f8c23-3d3c-488f-a71a-2a7abeb8f952",
   "metadata": {},
   "source": [
    "## 接下来，我们将介绍`Memory`的复杂应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21041ebb-54e4-45d9-8ce5-1233ea31fae4",
   "metadata": {},
   "source": [
    "基于上述介绍的Memory，我们现在可以构建一个简单的`RAG Agent`。\n",
    "\n",
    "在构建AI系统中常见的RAG（检索增强生成）模式包含两个不同的阶段：\n",
    " \n",
    "1. 索引：加载文档，将它们分块，并将它们存储在矢量数据库中\n",
    "\n",
    "2. 检索：在会话运行时查找和使用相关块\n",
    "\n",
    "在前面的示例中，我们手动将项添加到内存中并将它们传递给代理。在实践中，索引过程通常是自动化的，并且基于更大的文档源，如产品文档、内部文件或知识库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bb6b2-86b1-496f-b27e-2dd8624f15bf",
   "metadata": {},
   "source": [
    " 首先，让我们创建一个简单的`文档索引器`，我们将使用它来加载文档、对它们进行分组并将它们存储在内存存储区中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990e922d-7084-4c3d-837d-4e76071f4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n",
    "\n",
    "\n",
    "class SimpleDocumentIndexer:\n",
    "    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n",
    "\n",
    "    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n",
    "        self.memory = memory\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    async def _fetch_content(self, source: str) -> str:\n",
    "        \"\"\"Fetch content from URL or file.\"\"\"\n",
    "        if source.startswith((\"http://\", \"https://\")):\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(source) as response:\n",
    "                    return await response.text()\n",
    "        else:\n",
    "            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n",
    "                return await f.read()\n",
    "\n",
    "    def _strip_html(self, text: str) -> str:\n",
    "        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n",
    "        text = re.sub(r\"<[^>]*>\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into fixed-size chunks.\"\"\"\n",
    "        chunks: list[str] = []\n",
    "        # Just split text into fixed-size chunks\n",
    "        for i in range(0, len(text), self.chunk_size):\n",
    "            chunk = text[i : i + self.chunk_size]\n",
    "            chunks.append(chunk.strip())\n",
    "        return chunks\n",
    "        \n",
    "    async def index_documents(self, sources: List[str]) -> int:\n",
    "        \"\"\"Index documents into memory.\"\"\"\n",
    "        total_chunks = 0\n",
    "        for source in sources:\n",
    "            try:\n",
    "                content = await self._fetch_content(source)\n",
    "                # Strip HTML if content appears to be HTML\n",
    "                if \"<\" in content and \">\" in content:\n",
    "                    content = self._strip_html(content)\n",
    "                chunks = self._split_text(content)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    await self.memory.add(\n",
    "                        MemoryContent(\n",
    "                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                total_chunks += len(chunks)\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing {source}: {str(e)}\")\n",
    "\n",
    "        return total_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdd5ec-a5ab-465d-9215-67f7f6ee2f4c",
   "metadata": {},
   "source": [
    "现在让我们使用我们的索引器`ChromaDBVectorMemory`来构建一个完整的RAG Agent："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b1650-9870-41bb-93b7-e4263e895686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.memory import chromadb\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Initialize vector memory\n",
    "\n",
    "rag_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"autogen_docs\",\n",
    "        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),  #设置存储的硬盘区域\n",
    "        k=3,  # Return top 3 results\n",
    "        score_threshold=0.4,  # Minimum similarity score\n",
    "    )\n",
    ")\n",
    "\n",
    "await rag_memory.clear()  # Clear existing memory\n",
    "\n",
    "\n",
    "# Index AutoGen documentation\n",
    "async def index_autogen_docs() -> None:\n",
    "    indexer = SimpleDocumentIndexer(memory=rag_memory)\n",
    "    sources = [\n",
    "        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\"\n",
    "    ]\n",
    "    chunks: int = await indexer.index_documents(sources)\n",
    "    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n",
    "\n",
    "\n",
    "await index_autogen_docs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41da24-fef3-4b0e-8ce5-376cade18155",
   "metadata": {},
   "source": [
    "上述操作后，我们已经成功把我们传入的文档存储在ChromaDBVector中，现在我们准备讲改Memory集成到智能体之中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e51149-acf0-420d-a173-d1db08e2f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our RAG assistant agent\n",
    "rag_assistant = AssistantAgent(\n",
    "    name=\"rag_assistant\", model_client=model_client, memory=[rag_memory]\n",
    ")\n",
    "\n",
    "# Ask questions about AutoGen\n",
    "stream = rag_assistant.run_stream(task=\"What is AgentChat?\")\n",
    "await Console(stream)\n",
    "\n",
    "# Remember to close the memory when done\n",
    "await rag_memory.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49958537-29f5-4856-b2da-f1403c1bcb26",
   "metadata": {},
   "source": [
    "这个实现提供了一个RAG代理，可以根据AutoGen文档回答问题。当提出问题时，Memory系统检索相关的块并将它们添加到上下文中，使助手能够生成知情的回答。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
