{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64576468",
   "metadata": {},
   "source": [
    "# Memory \n",
    "\n",
    "åœ¨è®¸å¤šåº”ç”¨åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ç»´æŠ¤ä¸€ä¸ªâ€œçŸ¥è¯†å­˜å‚¨â€ï¼Œåœ¨ç‰¹å®šæ­¥éª¤ä¹‹å‰æ™ºèƒ½åœ°å°†å…¶ä¸­çš„äº‹å®åŠ å…¥æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "ä¸€ä¸ªå…¸å‹çš„ä¾‹å­å°±æ˜¯ RAG æ¨¡å¼ï¼ˆRetrieval-Augmented Generationï¼‰â€”â€”é€šè¿‡æŸ¥è¯¢æ£€ç´¢æ•°æ®åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œç„¶åå°†å…¶åŠ¨æ€æ³¨å…¥åˆ°æ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "\n",
    "AutoGen æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ `Memory` åè®®æ¥å£ï¼Œæ¥å®ç°ä¸Šè¿°èƒ½åŠ›ã€‚å®ƒåŒ…æ‹¬ä»¥ä¸‹å…³é”®æ–¹æ³•ï¼š\n",
    "\n",
    "| æ–¹æ³•å              | åŠŸèƒ½è¯´æ˜                                                 |\n",
    "| ---------------- | ---------------------------------------------------------  |\n",
    "| `add`            | å‘è®°å¿†åº“ä¸­æ·»åŠ æ–°æ¡ç›®                                         |\n",
    "| `query`          | ä»è®°å¿†åº“ä¸­æ£€ç´¢ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯                            |\n",
    "| `update_context` | ä¿®æ”¹æ™ºèƒ½ä½“çš„ `model_context`ï¼Œå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯åŠ¨æ€åŠ å…¥ï¼ˆç”± `AssistantAgent` ä½¿ç”¨ï¼‰|\n",
    "| `clear`          | æ¸…ç©ºè®°å¿†åº“ä¸­çš„æ‰€æœ‰å†…å®¹                                     |\n",
    "| `close`          | æ¸…ç†è®°å¿†åº“ä½¿ç”¨çš„èµ„æºï¼ˆå¦‚æ–‡ä»¶å¥æŸ„ã€æ•°æ®åº“è¿æ¥ç­‰ï¼‰            |\n",
    "\n",
    "ç›®å‰å®˜æ–¹æ–‡æ¡£ä¸­å·²æœ‰çš„é’ˆå¯¹ä¸åŒå­˜å‚¨ç±»å‹çš„Memoryç±»ï¼š\n",
    "\n",
    "1. ListMemory  \n",
    "\n",
    "`ListMemory` ç®€å•çš„åŸºäºåˆ—è¡¨çš„å†…å­˜å®ç° Memory åŠŸèƒ½ï¼ŒæŒ‰æ—¶é—´é¡ºåºç»´æŠ¤å†…å­˜ï¼Œå¹¶å°†æœ€è¿‘çš„å†…å­˜è¿½åŠ åˆ°æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸­ã€‚è¯¥å®ç°è®¾è®¡å¾—ç®€å•æ˜äº†ã€å¯é¢„æµ‹ï¼Œå› æ­¤æ˜“äºç†è§£å’Œè°ƒè¯•ã€‚ \n",
    "\n",
    "2. ChromaDBVectorMemory\n",
    "\n",
    "`ChromaDBVectorMemory`ä½¿ç”¨ç”± ChromaDB æ”¯æŒçš„çŸ¢é‡ç›¸ä¼¼æ€§æœç´¢å­˜å‚¨å’Œæ£€ç´¢å†…å­˜ã€‚ChromaDBVectorMemory \"æä¾›äº†ä¸€ç§åŸºäºå‘é‡çš„å†…å­˜å®ç°ï¼Œå®ƒä½¿ç”¨ ChromaDBå­˜å‚¨å’Œæ£€ç´¢åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„å†…å®¹ã€‚ å®ƒå¢å¼ºäº†ä»£ç†åœ¨å¯¹è¯è¿‡ç¨‹ä¸­è°ƒç”¨ä¸Šä¸‹æ–‡ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚   \n",
    "\n",
    "3. Mem0Memory\n",
    "\n",
    "`Mem0Memory` æä¾›ä¸ Mem0.ai å†…å­˜ç³»ç»Ÿçš„é›†æˆã€‚ å®ƒæ”¯æŒäº‘ç«¯å’Œæœ¬åœ°åç«¯ï¼Œä¸ºä»£ç†æä¾›é«˜çº§å†…å­˜åŠŸèƒ½ã€‚ è¯¥å®ç°å¯å¤„ç†é€‚å½“çš„æ£€ç´¢å’Œä¸Šä¸‹æ–‡æ›´æ–°ï¼Œå› æ­¤é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚\n",
    "\n",
    "é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ç»§æ‰¿`Memory`åŸºç±»ä»¥åŠé‡å†™å…¶ä¸­çš„æ–¹æ³•æ¥å®ç°æˆ‘ä»¬æƒ³è¦å­˜å‚¨è®°å¿†æ–¹å¼ï¼Œä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å®ç°ä¸€ä¸ªä½¿ç”¨å‘é‡æ•°æ®åº“æ¥å­˜å‚¨å’Œæ£€ç´¢ä¿¡æ¯çš„è‡ªå®šä¹‰å­˜å‚¨ç©ºé—´ï¼Œæˆ–è€…ä¸€ä¸ªä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ¥æ ¹æ®ç”¨æˆ·åå¥½ç­‰ç”Ÿæˆä¸ªæ€§åŒ–å“åº”çš„å­˜å‚¨ç©ºé—´ã€‚æˆ‘ä»¬å°†ä¼šä»¥æ„å»ºåŸºäº`Vector DBs`çš„å­˜å‚¨æ–¹å¼çš„Memoryç³»ç»Ÿä¸ºä¾‹å¯¹è¿™ä¸€é‡è¦éƒ¨åˆ†è¿›è¡Œè®²è§£ã€‚\n",
    "\n",
    "å¦å¤–ï¼Œæˆ‘ä»¬å¯åŸºäºMemoryçš„åŠŸèƒ½ä¸‹\n",
    "âœ… åº”ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n",
    "- ğŸŒ RAG æŸ¥è¯¢æ–‡æ¡£æ•°æ®åº“ï¼Œæ”¯æŒé—®ç­”æˆ–æ‘˜è¦ä»»åŠ¡\n",
    "\n",
    "- ğŸ¤– å°†è¿‡å»å¯¹è¯ç‰‡æ®µæˆ–ç”¨æˆ·è®¾å®šåŠ¨æ€æ³¨å…¥åˆ°å½“å‰è½®ä¸Šä¸‹æ–‡\n",
    "\n",
    "- ğŸ“ å°†ç”¨æˆ·ä¸Šä¼ çš„çŸ¥è¯†åº“ï¼ˆå¦‚ PDFã€ç½‘é¡µï¼‰ä½œä¸ºè®°å¿†æºåŠ¨æ€æ¥å…¥\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b683d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "load_dotenv()\n",
    "siliconflow_api_key = os.getenv(\"SILICONFLOW_API_KEY\") # è¯»å–ä½ çš„ OPENAI API key\n",
    "\n",
    "# åˆå§‹åŒ– OpenAIChatCompletionClient å®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°ç¡…åŸºæµåŠ¨å¹³å°çš„ Qwen3-8B æ¨¡å‹\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"Qwen/Qwen3-14B\",                # æŒ‡å®šè¦è°ƒç”¨çš„æ¨¡å‹åç§°ï¼Œç¡…åŸºæµåŠ¨å¹³å°ä¸Š Qwen 3-8B æ¨¡å‹\n",
    "    base_url=\"https://api.siliconflow.cn/v1\",  # ç¡…åŸºæµåŠ¨å¹³å°çš„ API è®¿é—®åœ°å€\n",
    "    api_key=siliconflow_api_key,  # ä½ çš„ API å¯†é’¥\n",
    "    model_info={                        \n",
    "        \"family\": \"qwen\",              \n",
    "        \"context_length\": 8192,        \n",
    "        \"max_output_tokens\": 2048,     \n",
    "        \"tool_choice_supported\": True, \n",
    "        \"tool_choice_required\": False,  \n",
    "        \"structured_output\": True,     \n",
    "        \"vision\": False,                \n",
    "        \"function_calling\": True,      \n",
    "        \"json_output\": True,\n",
    "        \"multiple_system_messages\":True\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f24e85-3646-4992-b69d-e825b38c7c1a",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬è¦ä»‹ç»autogen_core.memoryä¸‹çš„ä¸¤ä¸ªç±»`ListMemoryConfig`å’Œ `ListMemory`ï¼Œæ–¹ä¾¿ä¹‹åçš„å…¶ä»–ç±»å‹çš„Memoryè®²è§£\n",
    "\n",
    "### ListMemoryConfig\n",
    "\n",
    "`ListMemoryConfig`æ˜¯ Autogen æ¡†æ¶ä¸­ListMemoryç»„ä»¶çš„é…ç½®ç±»ï¼ŒåŸºäº Pydantic çš„`BaseModel`æ„å»ºï¼Œç”¨äºéªŒè¯å’Œåºåˆ—åŒ–å†…å­˜ç»„ä»¶çš„é…ç½®å‚æ•°ã€‚å…¶æ ¸å¿ƒä½œç”¨æ˜¯ï¼š\n",
    "\n",
    "1. å®šä¹‰ListMemoryçš„åˆå§‹åŒ–å‚æ•°è§„èŒƒ\n",
    "2. æ”¯æŒé…ç½®çš„åºåˆ—åŒ–ä¸ååºåˆ—åŒ–ï¼ˆå¦‚ JSON è½¬æ¢ï¼‰\n",
    "3. æä¾›ç±»å‹æç¤ºå’Œå‚æ•°éªŒè¯\n",
    "\n",
    "å…³é”®å±æ€§è¯´æ˜ï¼š\n",
    "\n",
    "`name`å±æ€§\n",
    "\n",
    "ç±»å‹ï¼šstr | None\n",
    "\n",
    "ä½œç”¨ï¼šä¸ºå†…å­˜å®ä¾‹æŒ‡å®šå”¯ä¸€åç§°ï¼Œä¾¿äºåœ¨å¤šç»„ä»¶åœºæ™¯ä¸­è¯†åˆ«\n",
    "\n",
    "é»˜è®¤å€¼ï¼šNoneï¼ˆè‡ªåŠ¨ç”Ÿæˆé»˜è®¤åç§°\"default_list_memory\"ï¼‰\n",
    "\n",
    "`memory_contents`å±æ€§\n",
    "\n",
    "ç±»å‹ï¼šList[MemoryContent]\n",
    "\n",
    "ä½œç”¨ï¼šåˆå§‹åŒ–æ—¶é¢„åŠ è½½çš„å†…å­˜å†…å®¹åˆ—è¡¨\n",
    "\n",
    "é»˜è®¤å€¼ï¼šé€šè¿‡default_factory=listç”Ÿæˆç©ºåˆ—è¡¨\n",
    "\n",
    "æ³¨æ„ï¼šMemoryContentæ˜¯ Autogen çš„å†…å­˜å†…å®¹æ¨¡å‹ï¼ŒåŒ…å«contentã€metadataã€mime_typeç­‰å­—æ®µ\n",
    "\n",
    "æˆ‘ä»¬æ¥çœ‹å®˜æ–¹æ–‡æ¡£é‡Œé¢çš„æºä»£ç ä¼šå¯¹å…¶å®šä¹‰æœ‰æ›´å¥½çš„äº†è§£ï¼š\n",
    "\n",
    "class ListMemoryConfig(BaseModel):\n",
    "    \"\"\"Configuration for ListMemory component.\"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    \"\"\"Optional identifier for this memory instance.\"\"\"\n",
    "    memory_contents: List[MemoryContent] = Field(default_factory=list)\n",
    "    \"\"\"List of memory contents stored in this memory instance.\"\"\"\n",
    "\n",
    "### ListMemory\n",
    "\n",
    "`ListMemory`æ˜¯ Autogen æ¡†æ¶ä¸­åŸºäºåˆ—è¡¨çš„å†…å­˜å®ç°ï¼Œç”¨äºå­˜å‚¨å’Œç®¡ç†è®°å¿†å†…å®¹ï¼Œéµå¾ªMemoryåè®®å¹¶é›†æˆComponenté…ç½®ç³»ç»Ÿã€‚å®ƒä»¥ chronologicalï¼ˆæ—¶é—´é¡ºåºï¼‰æ–¹å¼å­˜å‚¨å†…å®¹ï¼Œå¹¶æ”¯æŒå°†è®°å¿†å†…å®¹æ•´åˆåˆ°æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "\n",
    "æºä»£ç ä¸­å¯¹`ListMemory`ç±»çš„æ„é€ å‡½æ•°å¦‚ä¸‹ï¼š\n",
    "\n",
    "def __init__(self, name: str | None = None, memory_contents: List[MemoryContent] | None = None) -> None:\n",
    "        self._name = name or \"default_list_memory\"\n",
    "        self._contents: List[MemoryContent] = memory_contents if memory_contents is not None else []\n",
    "\n",
    "å…¶ä¸­ç¬¬äºŒä¸ªå‚æ•°memory_contentså°±å¯ä»¥ä¼ å…¥ä¸Šé¢æˆ‘ä»¬æ‰€è®²çš„é’ˆå¯¹ListMemoryçš„ç»Ÿä¸€é…ç½®`ListMemoryConfig`ï¼Œä½¿å¾—åˆå§‹åŒ–çš„ListMemoryå®ä¾‹éƒ½èƒ½è·å¾—ListMemoryConfigå·²ç»é…å¤‡å¥½çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¸¦`ListMemory`è®°å¿†åŠŸèƒ½çš„æ™ºèƒ½ä½“è¿ç”¨ä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563cb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "# Initialize user memory\n",
    "user_memory = ListMemory()\n",
    "\n",
    "# Add user preferences to memory\n",
    "await user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "\n",
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 Â°F and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 Â°C and Sunny.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't know the weather in {city}.\"\n",
    "\n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    memory=[user_memory],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e35587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is the weather in New York?\n",
      "---------- MemoryQueryEvent (assistant_agent) ----------\n",
      "[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n",
      "---------- ThoughtEvent (assistant_agent) ----------\n",
      "\n",
      "\n",
      "\n",
      "---------- ToolCallRequestEvent (assistant_agent) ----------\n",
      "[FunctionCall(id='0197c0adc898da07ad2e5e7026bad2dc', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')]\n",
      "---------- ToolCallExecutionEvent (assistant_agent) ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 23 Â°C and Sunny.', name='get_weather', call_id='0197c0adc898da07ad2e5e7026bad2dc', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (assistant_agent) ----------\n",
      "The weather in New York is 23 Â°C and Sunny.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='What is the weather in New York?', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, metadata={}, content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)], type='MemoryQueryEvent'), ThoughtEvent(source='assistant_agent', models_usage=None, metadata={}, content='\\n\\n', type='ThoughtEvent'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=733, completion_tokens=118), metadata={}, content=[FunctionCall(id='0197c0adc898da07ad2e5e7026bad2dc', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='The weather in New York is 23 Â°C and Sunny.', name='get_weather', call_id='0197c0adc898da07ad2e5e7026bad2dc', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, metadata={}, content='The weather in New York is 23 Â°C and Sunny.', type='ToolCallSummaryMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent with a task.\n",
    "stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c158eae1-fa6b-4202-9381-ae5acd33756e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n",
       " SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n",
       " AssistantMessage(content=[FunctionCall(id='0197c09c432fe83c0acceea5353fc584', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')], thought='\\n\\n', source='assistant_agent', type='AssistantMessage'),\n",
       " FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 Â°C and Sunny.', name='get_weather', call_id='0197c09c432fe83c0acceea5353fc584', is_error=False)], type='FunctionExecutionResultMessage'),\n",
       " UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n",
       " SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n",
       " AssistantMessage(content=[FunctionCall(id='0197c09f2125fe930799ce7cf4943125', arguments=' {\"city\": \"New York\", \"units\": \"metric\"}', name='get_weather')], thought='\\n\\n', source='assistant_agent', type='AssistantMessage'),\n",
       " FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 Â°C and Sunny.', name='get_weather', call_id='0197c09f2125fe930799ce7cf4943125', is_error=False)], type='FunctionExecutionResultMessage')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant_agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa508df2",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œ`assistant_agent` çš„ `model_context` å®é™…ä¸Šå·²ç»è¢«æ£€ç´¢åˆ°çš„è®°å¿†æ¡ç›®æ›´æ–°äº†ã€‚`transform` æ–¹æ³•ç”¨äºå°†è¿™äº›è®°å¿†æ¡ç›®æ ¼å¼åŒ–ä¸ºä¸€ä¸ªä»£ç†å¯ä»¥ä½¿ç”¨çš„å­—ç¬¦ä¸²ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ˜¯å°†æ¯ä¸ªè®°å¿†æ¡ç›®çš„å†…å®¹ç®€å•åœ°è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e32485",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸Šè¿°è¿”å›çš„å¤©æ°”ä¿¡æ¯æ˜¯ä»¥æ‘„æ°åº¦ä¸ºå•ä½çš„ï¼Œæ­£å¦‚ç”¨æˆ·åå¥½ä¸­æ‰€è®¾å®šçš„é‚£æ ·ã€‚\n",
    "\n",
    "åŒæ ·åœ°ï¼Œå‡è®¾æˆ‘ä»¬æå‡ºä¸€ä¸ªå…³äºç”Ÿæˆé¥®é£Ÿè®¡åˆ’çš„é—®é¢˜ï¼Œä»£ç†ä¹Ÿèƒ½å¤Ÿä»è®°å¿†å­˜å‚¨ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æä¾›ä¸€ä¸ªä¸ªæ€§åŒ–çš„ï¼ˆç´ é£Ÿï¼‰å›åº”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd8d78-084b-4b7d-a17a-386789dc3a58",
   "metadata": {},
   "source": [
    "è®²å®Œäº†ListMemoryçš„åŸºæœ¬ç”¨æ³•ï¼Œæˆ‘ä»¬ç®€å•ä¸¾ä¸€ä¸‹ `ChromaDBVectorMemory` å’Œ `Mem0Memory` çš„ä¾‹å­ï¼Œä»–ä»¬çš„ç”¨æ³•å¤§åŒå°å¼‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862cee7-247d-4bb5-bc82-319e7b303a33",
   "metadata": {},
   "source": [
    "### ChromaDBVectorMemory\n",
    "\n",
    "`ChromaDBVectorMemory`å°†æˆ‘ä»¬ä¼ å…¥çš„çŸ¥è¯†åº“è¿›è¡Œå‘é‡åŒ–åå­˜å‚¨åœ¨ChromaDBä¸­ï¼Œæ‰€ä»¥è¦ä¼ å…¥ä½ æƒ³è¦çš„Embeddingæ–¹æ³•ï¼Œç›®å‰å®˜æ–¹æºä»£ç ä¸­å®šä¹‰äº†ä»¥ä¸‹å‡ ä¸ªEmbeddingFunctionConfig\n",
    "\n",
    "1. SentenceTransformerEmbeddingFunctionConfig\n",
    "\n",
    "2. OpenAIEmbeddingFunctionConfig\n",
    "\n",
    "3. DefaultEmbeddingFunctionConfig\n",
    "\n",
    "4. CustomEmbeddingFunctionConfig\n",
    "\n",
    "ä»–ä»¬çš„åŒºåˆ«æ˜¯æ‰€ä½¿ç”¨çš„Embeddingæ¨¡å‹ä¸ä¸€æ ·ã€‚\n",
    "\n",
    "å»ºè®®ç”¨æˆ·ç›´æ¥å‰å¾€autogen_ext/memory/chromadb/_chroma_configs.py(V0.6.2)ä¸­æŸ¥çœ‹æºä»£ç ä¸­çš„å®šä¹‰ï¼Œè¿™æ ·ä½ ä¼šæœ‰æ›´æ¸…æ™°çš„ç†è§£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661f41e-0dc8-4694-8dc7-6bc88c8590e5",
   "metadata": {},
   "source": [
    "### ChromaDBVectorMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3fe02-2a82-4237-9caf-cabf01af56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import MemoryContent, MemoryMimeType\n",
    "from autogen_ext.memory import chromadb\n",
    "# (\n",
    "#     ChromaDBVectorMemory,\n",
    "#     PersistentChromaDBVectorMemoryConfig,\n",
    "#     SentenceTransformerEmbeddingFunctionConfig,\n",
    "# )\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Use a temporary directory for ChromaDB persistence\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    chroma_user_memory = ChromaDBVectorMemory(\n",
    "        config=PersistentChromaDBVectorMemoryConfig(\n",
    "            collection_name=\"preferences\",\n",
    "            persistence_path=tmpdir,  # Use the temp directory here\n",
    "            k=2,  # Return top k results\n",
    "            score_threshold=0.4,  # Minimum similarity score\n",
    "            embedding_function_config=SentenceTransformerEmbeddingFunctionConfig(\n",
    "                model_name=\"all-MiniLM-L6-v2\"  # Use default model for testing\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # Add user preferences to memory\n",
    "    await chroma_user_memory.add(\n",
    "        MemoryContent(\n",
    "            content=\"The weather should be in metric units\",\n",
    "            mime_type=MemoryMimeType.TEXT,\n",
    "            metadata={\"category\": \"preferences\", \"type\": \"units\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    await chroma_user_memory.add(\n",
    "        MemoryContent(\n",
    "            content=\"Meal recipe must be vegan\",\n",
    "            mime_type=MemoryMimeType.TEXT,\n",
    "            metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    # Create assistant agent with ChromaDB memory\n",
    "    assistant_agent = AssistantAgent(\n",
    "        name=\"assistant_agent\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_weather],\n",
    "        memory=[chroma_user_memory],\n",
    "    )\n",
    "\n",
    "    stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n",
    "    await Console(stream)\n",
    "\n",
    "    # await model_client.close()\n",
    "    # await chroma_user_memory.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c22a9a-81c7-4dcb-9968-f0879e374b4b",
   "metadata": {},
   "source": [
    "### Mem0Memory\n",
    "\n",
    "æ³¨æ„ï¼šMem0Memoryåœ¨0.6.2åŠä¹‹åçš„ç‰ˆæœ¬æ‰æœ‰çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90fa9e-4b1f-4461-ae2c-6afb4ffb8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import uuid\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, TypedDict, cast\n",
    "\n",
    "from autogen_core import CancellationToken, Component, ComponentBase\n",
    "from autogen_core.memory import Memory, MemoryContent, MemoryQueryResult, UpdateContextResult\n",
    "from autogen_core.model_context import ChatCompletionContext\n",
    "from autogen_core.models import SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Self\n",
    "\n",
    "try:\n",
    "    from mem0 import Memory as Memory0\n",
    "    from mem0 import MemoryClient\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"`mem0ai` not installed. Please install it with `pip install mem0ai`\") from e\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
    " # Create a cloud Mem0Memory\n",
    "memory = Mem0Memory(is_cloud=True)\n",
    "# Add something to memory\n",
    "await memory.add(MemoryContent(content=\"Important information to remember\"))\n",
    "# Retrieve memories with a search query\n",
    "results = await memory.query(\"relevant information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f8c23-3d3c-488f-a71a-2a7abeb8f952",
   "metadata": {},
   "source": [
    "## æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»`Memory`çš„å¤æ‚åº”ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21041ebb-54e4-45d9-8ce5-1233ea31fae4",
   "metadata": {},
   "source": [
    "åŸºäºä¸Šè¿°ä»‹ç»çš„Memoryï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºä¸€ä¸ªç®€å•çš„`RAG Agent`ã€‚\n",
    "\n",
    "åœ¨æ„å»ºAIç³»ç»Ÿä¸­å¸¸è§çš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ¨¡å¼åŒ…å«ä¸¤ä¸ªä¸åŒçš„é˜¶æ®µï¼š\n",
    " \n",
    "1. ç´¢å¼•ï¼šåŠ è½½æ–‡æ¡£ï¼Œå°†å®ƒä»¬åˆ†å—ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨çŸ¢é‡æ•°æ®åº“ä¸­\n",
    "\n",
    "2. æ£€ç´¢ï¼šåœ¨ä¼šè¯è¿è¡Œæ—¶æŸ¥æ‰¾å’Œä½¿ç”¨ç›¸å…³å—\n",
    "\n",
    "åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å°†é¡¹æ·»åŠ åˆ°å†…å­˜ä¸­å¹¶å°†å®ƒä»¬ä¼ é€’ç»™ä»£ç†ã€‚åœ¨å®è·µä¸­ï¼Œç´¢å¼•è¿‡ç¨‹é€šå¸¸æ˜¯è‡ªåŠ¨åŒ–çš„ï¼Œå¹¶ä¸”åŸºäºæ›´å¤§çš„æ–‡æ¡£æºï¼Œå¦‚äº§å“æ–‡æ¡£ã€å†…éƒ¨æ–‡ä»¶æˆ–çŸ¥è¯†åº“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bb6b2-86b1-496f-b27e-2dd8624f15bf",
   "metadata": {},
   "source": [
    " é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„`æ–‡æ¡£ç´¢å¼•å™¨`ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥åŠ è½½æ–‡æ¡£ã€å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç»„å¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨å†…å­˜å­˜å‚¨åŒºä¸­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990e922d-7084-4c3d-837d-4e76071f4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n",
    "\n",
    "\n",
    "class SimpleDocumentIndexer:\n",
    "    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n",
    "\n",
    "    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n",
    "        self.memory = memory\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    async def _fetch_content(self, source: str) -> str:\n",
    "        \"\"\"Fetch content from URL or file.\"\"\"\n",
    "        if source.startswith((\"http://\", \"https://\")):\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(source) as response:\n",
    "                    return await response.text()\n",
    "        else:\n",
    "            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n",
    "                return await f.read()\n",
    "\n",
    "    def _strip_html(self, text: str) -> str:\n",
    "        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n",
    "        text = re.sub(r\"<[^>]*>\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into fixed-size chunks.\"\"\"\n",
    "        chunks: list[str] = []\n",
    "        # Just split text into fixed-size chunks\n",
    "        for i in range(0, len(text), self.chunk_size):\n",
    "            chunk = text[i : i + self.chunk_size]\n",
    "            chunks.append(chunk.strip())\n",
    "        return chunks\n",
    "        \n",
    "    async def index_documents(self, sources: List[str]) -> int:\n",
    "        \"\"\"Index documents into memory.\"\"\"\n",
    "        total_chunks = 0\n",
    "        for source in sources:\n",
    "            try:\n",
    "                content = await self._fetch_content(source)\n",
    "                # Strip HTML if content appears to be HTML\n",
    "                if \"<\" in content and \">\" in content:\n",
    "                    content = self._strip_html(content)\n",
    "                chunks = self._split_text(content)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    await self.memory.add(\n",
    "                        MemoryContent(\n",
    "                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                total_chunks += len(chunks)\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing {source}: {str(e)}\")\n",
    "\n",
    "        return total_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdd5ec-a5ab-465d-9215-67f7f6ee2f4c",
   "metadata": {},
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ç´¢å¼•å™¨`ChromaDBVectorMemory`æ¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„RAG Agentï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b1650-9870-41bb-93b7-e4263e895686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.memory import chromadb\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Initialize vector memory\n",
    "\n",
    "rag_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"autogen_docs\",\n",
    "        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),  #è®¾ç½®å­˜å‚¨çš„ç¡¬ç›˜åŒºåŸŸ\n",
    "        k=3,  # Return top 3 results\n",
    "        score_threshold=0.4,  # Minimum similarity score\n",
    "    )\n",
    ")\n",
    "\n",
    "await rag_memory.clear()  # Clear existing memory\n",
    "\n",
    "\n",
    "# Index AutoGen documentation\n",
    "async def index_autogen_docs() -> None:\n",
    "    indexer = SimpleDocumentIndexer(memory=rag_memory)\n",
    "    sources = [\n",
    "        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\"\n",
    "    ]\n",
    "    chunks: int = await indexer.index_documents(sources)\n",
    "    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n",
    "\n",
    "\n",
    "await index_autogen_docs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41da24-fef3-4b0e-8ce5-376cade18155",
   "metadata": {},
   "source": [
    "ä¸Šè¿°æ“ä½œåï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸæŠŠæˆ‘ä»¬ä¼ å…¥çš„æ–‡æ¡£å­˜å‚¨åœ¨ChromaDBVectorä¸­ï¼Œç°åœ¨æˆ‘ä»¬å‡†å¤‡è®²æ”¹Memoryé›†æˆåˆ°æ™ºèƒ½ä½“ä¹‹ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e51149-acf0-420d-a173-d1db08e2f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our RAG assistant agent\n",
    "rag_assistant = AssistantAgent(\n",
    "    name=\"rag_assistant\", model_client=model_client, memory=[rag_memory]\n",
    ")\n",
    "\n",
    "# Ask questions about AutoGen\n",
    "stream = rag_assistant.run_stream(task=\"What is AgentChat?\")\n",
    "await Console(stream)\n",
    "\n",
    "# Remember to close the memory when done\n",
    "await rag_memory.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49958537-29f5-4856-b2da-f1403c1bcb26",
   "metadata": {},
   "source": [
    "è¿™ä¸ªå®ç°æä¾›äº†ä¸€ä¸ªRAGä»£ç†ï¼Œå¯ä»¥æ ¹æ®AutoGenæ–‡æ¡£å›ç­”é—®é¢˜ã€‚å½“æå‡ºé—®é¢˜æ—¶ï¼ŒMemoryç³»ç»Ÿæ£€ç´¢ç›¸å…³çš„å—å¹¶å°†å®ƒä»¬æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œä½¿åŠ©æ‰‹èƒ½å¤Ÿç”ŸæˆçŸ¥æƒ…çš„å›ç­”ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
